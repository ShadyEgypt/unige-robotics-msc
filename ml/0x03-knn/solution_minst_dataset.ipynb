{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small training set shape: (5000, 28, 28) (5000,)\n",
      "Small testing set shape: (5000, 28, 28) (5000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "# Set the sample size\n",
    "sample_size = 5000\n",
    "\n",
    "# Randomly select 5000 indices for train and test data\n",
    "train_indices = np.random.choice(len(train_X), sample_size, replace=False)\n",
    "test_indices = np.random.choice(len(test_X), sample_size, replace=False)\n",
    "\n",
    "# Subset the data\n",
    "small_train_X = train_X[train_indices]\n",
    "small_train_y = train_y[train_indices]\n",
    "small_test_X = test_X[test_indices]\n",
    "small_test_y = test_y[test_indices]\n",
    "\n",
    "# Output the shapes to verify\n",
    "print(\"Small training set shape:\", small_train_X.shape, small_train_y.shape)\n",
    "print(\"Small testing set shape:\", small_test_X.shape, small_test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and normalize the data\n",
    "small_train_X = small_train_X.reshape(small_train_X.shape[0], -1) / 255.0\n",
    "small_test_X = small_test_X.reshape(small_test_X.shape[0], -1) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classify_with_metrics(train_X, train_y, test_X, k, test_y):\n",
    "    \"\"\"\n",
    "    Classifies test set according to the k-Nearest Neighbors (kNN) rule and computes evaluation metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        train_X (ndarray): Training set feature matrix\n",
    "        train_y (ndarray): Training set labels\n",
    "        test_X (ndarray): Test set feature matrix\n",
    "        test_y (ndarray): True labels for the test set\n",
    "        k (int): Number of nearest neighbors to use for classification\n",
    "        \n",
    "    Returns:\n",
    "        test_predictions (ndarray): Predicted labels for the test set\n",
    "        metrics (dict): A dictionary containing evaluation metrics\n",
    "                        (Accuracy, Precision, Recall, F1 Score)\n",
    "    \"\"\"\n",
    "    # Check that k > 0 and k <= cardinality of the training set\n",
    "    n = train_X.shape[0]\n",
    "    if k <= 0 or k > n:\n",
    "        raise ValueError(f\"k must be greater than 0 and less than or equal to {n}\")\n",
    "\n",
    "    # Check that the number of columns in test_X equals the number of columns in train_X\n",
    "    if train_X.shape[1] != test_X.shape[1]:\n",
    "        raise ValueError(\"Number of columns in test set must match the training set\")\n",
    "\n",
    "    # Perform kNN classification\n",
    "    test_predictions = []\n",
    "    for test_sample in test_X:\n",
    "        # Compute distances from test_sample to all training samples\n",
    "        distances = np.linalg.norm(train_X - test_sample, axis=1)\n",
    "        \n",
    "        # Get the indices of the k nearest neighbors\n",
    "        neighbor_indices = np.argsort(distances)[:k]\n",
    "        \n",
    "        # Get the labels of the k nearest neighbors\n",
    "        neighbor_labels = train_y[neighbor_indices]\n",
    "        \n",
    "        # Determine the most frequent label (mode) among the neighbors\n",
    "        predicted_label = mode(neighbor_labels, axis=None).mode  # Ensure axis=None for scalar input\n",
    "        predicted_label = predicted_label[0] if isinstance(predicted_label, np.ndarray) else predicted_label\n",
    "        \n",
    "        # Append the predicted label to the list\n",
    "        test_predictions.append(predicted_label)\n",
    "    \n",
    "    # Convert predictions to a NumPy array\n",
    "    test_predictions = np.array(test_predictions)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    if test_y is not None and len(test_y) > 0:  # Ensure test_y is not None and not empty\n",
    "        accuracy = accuracy_score(test_y, test_predictions)\n",
    "        precision = precision_score(test_y, test_predictions, average='weighted')\n",
    "        recall = recall_score(test_y, test_predictions, average='weighted')\n",
    "        f1 = f1_score(test_y, test_predictions, average='weighted')\n",
    "    else:\n",
    "        accuracy = 0\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "\n",
    "    \n",
    "\n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1\n",
    "    }\n",
    "\n",
    "    return test_predictions, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_rate(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes the error rate of predictions against the true targets.\n",
    "    \n",
    "    Parameters:\n",
    "        predictions (ndarray): Predicted labels\n",
    "        targets (ndarray): Actual labels\n",
    "        \n",
    "    Returns:\n",
    "        error_rate (float): Error rate calculated as number of errors divided by number of samples\n",
    "    \"\"\"\n",
    "    errors = np.sum(predictions != targets)\n",
    "    error_rate = errors / len(targets)\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "# Classify the test set\n",
    "predictions, metrics = knn_classify_with_metrics(small_train_X, small_train_y, small_test_X, k, small_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 0.0662\n"
     ]
    }
   ],
   "source": [
    "error_rate = compute_error_rate(predictions, small_test_y)\n",
    "print(f\"Error Rate: {error_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[481   0   0   0   0   3   4   0   0   0]\n",
      " [  0 566   1   0   0   0   0   0   0   0]\n",
      " [ 11  21 482   6   6   0   5   8   5   0]\n",
      " [  0   3   3 478   0   8   1   6   5   2]\n",
      " [  0   9   0   0 421   0   4   1   0  19]\n",
      " [  1   6   0  12   4 401   6   1   1   8]\n",
      " [  6   3   0   0   2   1 468   0   0   0]\n",
      " [  1  23   1   0   3   0   0 477   0   6]\n",
      " [  8  13   1  18   5  16   6   5 413   9]\n",
      " [  4   3   1   3  10   2   0  10   1 482]]\n"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(small_test_y, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Mean   Std Dev  25th Percentile  75th Percentile\n",
      "Accuracy   0.986760  0.003302         0.984850         0.987300\n",
      "Precision  0.936284  0.028629         0.926022         0.945390\n",
      "Recall     0.933187  0.045868         0.915351         0.967416\n"
     ]
    }
   ],
   "source": [
    "total_samples = np.sum(conf_matrix)\n",
    "\n",
    "# Initialize lists for metrics\n",
    "precisions = []\n",
    "recalls = []\n",
    "accuracies = []\n",
    "\n",
    "# Compute metrics for each class\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    TP = conf_matrix[i, i]\n",
    "    FP = np.sum(conf_matrix[:, i]) - TP\n",
    "    FN = np.sum(conf_matrix[i, :]) - TP\n",
    "    TN = total_samples - (TP + FP + FN)\n",
    "    \n",
    "    # Calculate precision, recall, and accuracy\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    accuracy = (TP + TN) / total_samples if total_samples > 0 else 0\n",
    "    \n",
    "    # Append metrics\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracies,\n",
    "    \"Precision\": precisions,\n",
    "    \"Recall\": recalls,\n",
    "}\n",
    "    \n",
    "# Compute statistics\n",
    "summary = {metric: {\n",
    "    \"Mean\": np.mean(values),\n",
    "    \"Std Dev\": np.std(values),\n",
    "    \"25th Percentile\": np.percentile(values, 25),\n",
    "    \"75th Percentile\": np.percentile(values, 75)\n",
    "} for metric, values in metrics.items()}\n",
    "\n",
    "# Print summary\n",
    "summary_df = pd.DataFrame(summary).T\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_to_csv(predictions, output_file=\"predictions.csv\"):\n",
    "    \"\"\"\n",
    "    Saves predictions to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        predictions (ndarray or list): Predicted labels for the test set\n",
    "        output_file (str): Name of the output CSV file (default: \"predictions.csv\")\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with predictions\n",
    "    df = pd.DataFrame(predictions, columns=[\"Prediction\"])\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "save_predictions_to_csv(predictions, output_file=\"predictions_minst_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
